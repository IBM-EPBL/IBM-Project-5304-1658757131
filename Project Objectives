. They have discarded the instances with missing values and trained a neural network and a logistic regression model which gave 0.975 and 0.960 of overall accuracies respectively. The co-relations of the selected attributes
vary in the range of [0.2 to 0.8]. Considering the medical perspective of attributes, hypertension can cause CKD as well as CKD causes hypertension and specific gravity has 0.73
co-relations to the class. Discarding such attributes could lower accuracy. . In 2015 Lambodar J. and Narendra Ku. K. have experimented with 8 machine learning models using
WEKA data mining tool

. Data Preprocessing
(1) Missing Value Handling - In the pre-processing step, missing values have to be handled based on their distributions to achieve reasonable
accuracy. In this work, to confirm the randomness of missing values, Little's MCAR test was performed. The potential bias
due to missing data depends on the mechanism causing the data to be missing. The analytical methods applied to amend
the missingness [15] are tested using the chi-square test of MCAR for multivariate quantitative data. It tests whether
there exists a significant difference between the means of different missing-value patterns.
(2) Feature Selection - In addition above mentioned factors, feasibility and the obtainability  was also considered in attributes selection.
The distribution of appetite values against the class shows that it biases towards good appetite (Fig. 7). However, CKD
is not the only reason to have a poor appetite, which will mislead the predictions when applying the trained model to a new scenario.
(3) Model Training - Those are the decision tree classifier, random forest classifier, XGB classifier, extra trees classifier,
ada boost classifier and classical neural network. The implementations and evaluation were done using Python Sci-kit, and Keras frameworks.
(4) Model Evaluation and Selection - Even though the mentioned models gave 100% accuracy, its important to identify attributes with the highest impact on
each model to make the decision. After identifying the importance of selected features for each prediction algorithm, the standard deviation of the
feature importance of each algorithm was calculated, which explicitly showed the algorithm's bias towards different attributes (Table VI, Table VII, Fig. 8).
The results (Table VI, Table VII, Fig. 8) show that the extra trees classifier has the lowest bias towards features next
to the random forest classifier. decision tree classifier has the highest bias out of all.

.Applying different algorithms 
(1)- Logistic regression: Logistic regression, also called logit model or logistic model, is a widely used model to
analyze the relationship between multiple independent variables and one categorical dependent variable with the equation of the form:
(2) - SVM: Support Vector Machines (SVM) is a supervised learning model that is commonly used in classification problems. The idea of the SVM algorithm is to
figure the optimal hyperplane that ideally separates all objects of one class from those objects of another class with the largest margins between these two classes.
(3) -  Ensemble method: Ensemble method is a strategy for improving predictor or classifier accuracy. Ensemble method uses a combination of models to create an improved
composite model to improve the performance. The main idea behind the ensemble technique is to group multiple “weak learners” to come up with a “strong learner”.
(4)- Random Forest: Random forest (RF) is a bagging ensemble approach proposed by Breiman  that based on a machine learning mechanism called “decision tree”. In a
random forest, the “weak learners” in ensemble terms are decision trees.
(5) - Gradient Boosting: Gradient boosting (GB) is an ensemble boosting technique that starts with “regression tree”
as “weak learners”. In general, the GB model adds an additive model to minimize the loss function by using a stage-wise sampling strategy.

. By using the above algorithms, the web application flask framework will be done accurately.
